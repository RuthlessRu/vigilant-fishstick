{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchaudio torchlibrosa librosa pandas matplotlib seaborn kagglehub\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchlibrosa as tl\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio.transforms as T\n",
    "import kagglehub\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"Current device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_path = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
    "\n",
    "emotions = []\n",
    "file_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(tess_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            emotion = os.path.basename(root) # take base name as emotion\n",
    "            emotions.append(emotion)\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "labels = [label.lower().split('_')[1] if '_' in label else label.lower() for label in emotions]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = onehot_encoder.fit_transform(encoded_labels.reshape(-1, 1))\n",
    "y_onehot = y_onehot.astype(np.float32)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    file_paths, y_onehot, test_size=0.2, stratify=y_onehot, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42\n",
    ")\n",
    "display(len(X_train))\n",
    "display(y_train.shape)\n",
    "\n",
    "display(len(X_val))\n",
    "display(y_val.shape)\n",
    "\n",
    "display(len(X_test))\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def noise(data, noise_factor_range=(0.01, 0.05)):\n",
    "    noise_factor = tf.random.uniform([], minval=noise_factor_range[0], maxval=noise_factor_range[1])\n",
    "    noise = noise_factor * tf.random.normal(tf.shape(data))\n",
    "    return data + noise\n",
    "\n",
    "def random_gain(data, gain_range=(0.8, 1.2)):\n",
    "    gain = tf.random.uniform([], minval=gain_range[0], maxval=gain_range[1])\n",
    "    return data * gain\n",
    "\n",
    "def augment_audio(data):\n",
    "    # Add random noise\n",
    "    data = tf.cond(tf.random.uniform([]) > 0.5, lambda: noise(data), lambda: data)\n",
    "    # Adjust gain\n",
    "    data = tf.cond(tf.random.uniform([]) > 0.5, lambda: random_gain(data), lambda: data)\n",
    "\n",
    "    return data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(waveform, threshold=1e-4):\n",
    "    \"\"\"\n",
    "    Trims silence from the waveform based on a threshold.\n",
    "    \"\"\"\n",
    "    # Reduce multi-channel waveform to a single channel for silence detection\n",
    "    if waveform.dim() > 1:\n",
    "        reduced_waveform = waveform.mean(dim=0)  # Take the mean of channels\n",
    "    else:\n",
    "        reduced_waveform = waveform\n",
    "\n",
    "    # Find non-silent indices\n",
    "    non_silent_indices = torch.where(reduced_waveform.abs() > threshold)[0]\n",
    "    \n",
    "    # If no non-silent indices are found, return the original waveform\n",
    "    if len(non_silent_indices) == 0:\n",
    "        return waveform\n",
    "\n",
    "    # Trim waveform to the range of non-silent indices\n",
    "    start, end = non_silent_indices[0], non_silent_indices[-1] + 1\n",
    "    return waveform[:, start:end] if waveform.dim() > 1 else waveform[start:end]\n",
    "\n",
    "def preprocess_audio(file_path, target_sr=16000, duration=2.5, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Preprocesses audio data for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the audio file.\n",
    "        target_sr: Target sampling rate.\n",
    "        duration: Desired duration of the audio clip in seconds.\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed audio tensor.\n",
    "    \"\"\"\n",
    "    waveform, sr = torchaudio.load(file_path) \n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = target_sr\n",
    "\n",
    "    # Trim silence\n",
    "    waveform = trim_silence(waveform)\n",
    "\n",
    "    # Normalize amplitude\n",
    "    waveform = waveform / waveform.abs().max()    \n",
    "\n",
    "    max_length = int(target_sr * duration)\n",
    "    if waveform.size(1) > max_length:\n",
    "        waveform = waveform[:, :max_length]  # Truncate\n",
    "    else:\n",
    "        pad_length = max_length - waveform.size(1)\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad_length))  # Pad\n",
    "\n",
    "    return waveform, sr\n",
    "\n",
    "def extract_mel_spectrogram(waveform, sr=16000, n_mels=128, fmax=8000, hop_length=512, device=\"cuda\"):\n",
    "    # Define the MelSpectrogram transform\n",
    "    mel_spectrogram_transform = T.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_mels=n_mels,\n",
    "        f_max=fmax,\n",
    "        hop_length=hop_length\n",
    "    ).to(device)\n",
    "\n",
    "    # Apply transform\n",
    "    mel_spectrogram = mel_spectrogram_transform(waveform)\n",
    "\n",
    "    # Convert to decibels (similar to librosa.power_to_db)\n",
    "    mel_spectrogram_db = T.AmplitudeToDB()(mel_spectrogram)\n",
    "\n",
    "    return mel_spectrogram_db\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, preprocess_fn, device=\"cuda\"):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Preprocess the audio file\n",
    "        waveform, _ = self.preprocess_fn(file_path, device=self.device)\n",
    "\n",
    "        # Extract mel spectrogram\n",
    "        mel_spectrogram = extract_mel_spectrogram(waveform, device=self.device)\n",
    "\n",
    "        return mel_spectrogram, torch.tensor(label, device=self.device)\n",
    "\n",
    "def create_pytorch_dataloader(file_paths, labels, preprocess_fn, batch_size=32, shuffle=True, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch DataLoader for audio data.\n",
    "\n",
    "    Args:\n",
    "        file_paths: List of file paths to audio files.\n",
    "        labels: List of corresponding labels.\n",
    "        preprocess_fn: Function for preprocessing audio.\n",
    "        batch_size: Batch size for the DataLoader.\n",
    "        shuffle: Whether to shuffle the data.\n",
    "        device: Device to perform processing (e.g., \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        PyTorch DataLoader object.\n",
    "    \"\"\"\n",
    "    dataset = AudioDataset(file_paths, labels, preprocess_fn, device=device)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_pytorch_dataloader(file_paths, labels, preprocess_audio, batch_size=32, shuffle=True, device=\"cuda\")\n",
    "\n",
    "train_loader = create_pytorch_dataloader(X_train, y_train, preprocess_audio, batch_size=32, shuffle=True, device=\"cuda\")\n",
    "val_loader = create_pytorch_dataloader(X_val, y_val, preprocess_audio, batch_size=32, shuffle=False, device=\"cuda\")\n",
    "test_loader = create_pytorch_dataloader(X_test, y_test, preprocess_audio, batch_size=32, shuffle=False, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Initializes the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The dimensionality of the input features.\n",
    "        \"\"\"\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.attention_layer = nn.Linear(input_dim, 1) \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Weighted input tensor of the same shape as inputs.\n",
    "        \"\"\"\n",
    "        # Apply linear transformation and tanh activation\n",
    "        attention = self.tanh(self.attention_layer(inputs))  # Shape: (batch_size, sequence_length, 1)\n",
    "        \n",
    "        # Flatten and apply softmax along the sequence dimension\n",
    "        attention = F.softmax(attention.squeeze(-1), dim=-1)  # Shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Expand dimensions to match inputs for element-wise multiplication\n",
    "        attention = attention.unsqueeze(-1)  # Shape: (batch_size, sequence_length, 1)\n",
    "        \n",
    "        # Compute the weighted inputs\n",
    "        weighted_inputs = inputs * attention  # Element-wise multiplication\n",
    "        return weighted_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRNNWithAttention(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(ConvRNNWithAttention, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Conv2D(32, (3, 3), padding='same')\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Conv2D(64, (3, 3), padding='same')\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)  # MaxPooling2D((2, 2))\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Reshape for RNN\n",
    "        self.reshape_dim = (input_shape[0] // 4) * (input_shape[1] // 4) * 64\n",
    "\n",
    "        # Bi-directional LSTMs\n",
    "        self.rnn1 = nn.LSTM(self.reshape_dim, 128, bidirectional=True, batch_first=True)  # LSTM(128, return_sequences=True)\n",
    "        self.rnn2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True)  # LSTM(64, return_sequences=True)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = AttentionMechanism(128)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 64)  # Dense(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # Dense(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Reshape for RNN input\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.reshape_dim)\n",
    "\n",
    "        # Bi-directional LSTMs\n",
    "        x, _ = self.rnn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.rnn2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device=\"cuda\", epochs=30):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels.argmax(dim=1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_accuracy = correct / total\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels.argmax(dim=1)).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss / len(train_loader):.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss / len(val_loader):.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels.argmax(dim=1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = ConvRNNWithAttention(input_shape=(128, 79, 1), num_classes=y_train.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss as the criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = create_pytorch_dataloader(X_train, y_train, preprocess_audio, batch_size=32, shuffle=True, device=\"cuda\")\n",
    "    val_loader = create_pytorch_dataloader(X_val, y_val, preprocess_audio, batch_size=32, shuffle=False, device=\"cuda\")\n",
    "    test_loader = create_pytorch_dataloader(X_test, y_test, preprocess_audio, batch_size=32, shuffle=False, device=\"cuda\")\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, device=\"cuda\", epochs=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_loader, criterion, device=\"cuda\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
