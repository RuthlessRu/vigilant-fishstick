{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an SER Emotion Classifier!\n",
    "\n",
    "In this project, we aim to build a SER emotion classifier using the TESS dataset provided by the University of Toronto!\n",
    "\n",
    "Our goal is to:\n",
    "1. Retrieve Data\n",
    "2. Explore Data\n",
    "3. Augment Data\n",
    "4. Process Data\n",
    "5. Extract Features\n",
    "6. Train Model\n",
    "7. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install seaborn\n",
    "#%pip install librosa\n",
    "#%pip install kagglehub\n",
    "#%pip install pandas\n",
    "#%pip install matplotlib\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense,\n",
    "    Bidirectional, LSTM, Reshape, Permute, Multiply, Lambda, Activation\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True) # TURNED ON FOR GPU STUFF YOU CAN TURN THIS OFF\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# import tensorflow as tf\n",
    "# print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "tess_path = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
    "\n",
    "print(\"Path to dataset files:\", tess_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = []\n",
    "file_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(tess_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            emotion = os.path.basename(root) # take base name as emotion\n",
    "            emotions.append(emotion)\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "    'Emotion': emotions,\n",
    "    'File_Path': file_paths\n",
    "})\n",
    "print(data_df['Emotion'].unique())\n",
    "\n",
    "# remove oaf and yaf\n",
    "data_df['Emotion'] = data_df['Emotion'].apply(lambda label: label.lower().split('_')[1] if '_' in label else label)\n",
    "\n",
    "print(data_df['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion distribution!\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.countplot(data=data_df, x='Emotion', order=data_df['Emotion'].value_counts().index)\n",
    "plt.title('Emotion Distribution in TESS Dataset', fontsize=16) # big font\n",
    "plt.xlabel('Emotions', fontsize=13) # bigger font for clarity\n",
    "plt.ylabel('Count', fontsize=13) # bigger font for clarity\n",
    "plt.xticks(rotation=45) # rotate them since the words are long\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration!\n",
    "\n",
    "Here, we understand and explore the TESS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    data, sr = librosa.load(file_path, sr=None) # audio with original sample\n",
    "    return data, sr\n",
    "\n",
    "def visualize_and_play(data, sr, title=\"Audio\", size=(14,5)):\n",
    "    \"\"\"\n",
    "    Visualize the waveform and play the audio.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=size)\n",
    "    plt.title(f\"Waveform - {title}\")\n",
    "    librosa.display.waveshow(data, sr=sr)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "    \n",
    "    return Audio(data, rate=sr)\n",
    "\n",
    "def plot_spectrogram(data, sr, title=\"Audio\", size=(10, 6)):\n",
    "    \"\"\"\n",
    "    Plot the mel spectrogram from audio data.\n",
    "    \"\"\"\n",
    "    spectrogram = librosa.feature.melspectrogram(y=data, sr=sr)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "    plt.figure(figsize=size)\n",
    "    plt.title(f\"Mel Spectrogram - {title}\")\n",
    "    librosa.display.specshow(spectrogram_db, sr=sr, x_axis='time', y_axis='mel', cmap='coolwarm')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.show()\n",
    "\n",
    "original_data, sr = load_audio(data_df.loc[0, 'File_Path'])\n",
    "audio_player = visualize_and_play(original_data, sr)\n",
    "plot_spectrogram(original_data, sr)\n",
    "audio_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_emotions = data_df['Emotion'].unique()\n",
    "for emotion in unique_emotions:\n",
    "    sample_file = data_df[data_df['Emotion'] == emotion].iloc[0]['File_Path']\n",
    "    original_data, sr = load_audio(sample_file)\n",
    "    plot_spectrogram(original_data, sr, size=(4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Since our dataset is quite small in size, we will use data augmentation strategies to help increase diversity.\n",
    "\n",
    "We will implement:\n",
    "1. Noise Injection\n",
    "2. Time Stretching\n",
    "3. Pitch Shifting\n",
    "\n",
    "We add elements of randomness to the augmentation, while also keeping a boundary to help better improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data, noise_factor_range=(0.01, 0.05)):\n",
    "    noise_factor = np.random.uniform(*noise_factor_range)  # randomness in noise\n",
    "    noise_amp = noise_factor * np.max(data)\n",
    "    return data + noise_amp * np.random.normal(size=data.shape)\n",
    "\n",
    "def stretch(data, rate_range=(0.8, 1.2)):\n",
    "    rate = np.random.uniform(*rate_range)  # random stretching\n",
    "    while rate == 1:  # if we get 1, make sure there is.\n",
    "        rate = np.random.uniform(*rate_range)\n",
    "    return librosa.effects.time_stretch(data, rate=rate)\n",
    "\n",
    "def pitch(data, sr, n_steps_range=(-4, 4)):\n",
    "    n_steps = np.random.randint(*n_steps_range)  # random shifting of pitch\n",
    "    while n_steps == 0: #if no pitch change, ensure it\n",
    "        n_steps = np.random.randint(*n_steps_range)\n",
    "    return librosa.effects.pitch_shift(data, sr=sr, n_steps=n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(data, sr):\n",
    "    augmented_data = [data]\n",
    "    augmented_data.append(noise(data))\n",
    "    augmented_data.append(stretch(data))\n",
    "    augmented_data.append(pitch(data, sr))\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = data_df.loc[0, 'File_Path']\n",
    "original_data, sr = load_audio(sample_file)\n",
    "\n",
    "augmented_versions = augment_audio(original_data, sr)\n",
    "titles = [\"Original\", \"Noise Added\", \"Time Stretched\", \"Pitch Shifted\"]\n",
    "\n",
    "for i, augmented_data in enumerate(augmented_versions):\n",
    "    print(f\"Playing: {titles[i]}\")\n",
    "    audio_player = visualize_and_play(augmented_data, sr, title=titles[i], size=(14,4))\n",
    "    display(audio_player)\n",
    "    plot_spectrogram(augmented_data, sr, title=titles[i], size=(4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(data_df):\n",
    "    \"\"\"\n",
    "    augment entire dataset by applying augmentation to each audio file.\n",
    "    \"\"\"\n",
    "    augmented_audio = []\n",
    "    emotions = []\n",
    "\n",
    "    for _, row in data_df.iterrows():\n",
    "        file_path = row['File_Path']\n",
    "        emotion = row['Emotion']\n",
    "        raw_data, sr = load_audio(file_path)\n",
    "        # augment\n",
    "        augmented_versions = augment_audio(raw_data, sr)\n",
    "        \n",
    "        # add to dataset\n",
    "        for augmented_data in augmented_versions:\n",
    "            augmented_audio.append(augmented_data)\n",
    "            emotions.append(emotion)  # Same label for all augmented versions\n",
    "\n",
    "    return augmented_audio, emotions, sr\n",
    "\n",
    "augmented_audio, augmented_emotions, sr = augment_dataset(data_df)\n",
    "augmented_data_df = pd.DataFrame({\n",
    "    'Emotion': augmented_emotions,\n",
    "    'Audio_Data': augmented_audio\n",
    "})\n",
    "data_df['Audio_Data'] = data_df['File_Path'].apply(lambda path: load_audio(path)[0])\n",
    "full_data_df = pd.concat([data_df[['Emotion', 'Audio_Data']], augmented_data_df], ignore_index=True)\n",
    "print(full_data_df.head())\n",
    "print(f\"Total samples in combined dataset: {len(full_data_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Preprocessing\n",
    "\n",
    "Now, we need to do 4 main things.\n",
    "1. Resample the audio to 16k\n",
    "2. Remove silence segments\n",
    "3. Keep a fixed duration\n",
    "4. Normalize the Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(data, sr, target_sr=16000, duration=2.5):\n",
    "    # resample ti 16k\n",
    "    if sr != target_sr:\n",
    "        data = librosa.resample(data, orig_sr=sr, target_sr=target_sr)\n",
    "    \n",
    "    # trim silence\n",
    "    data, _ = librosa.effects.trim(data)\n",
    "\n",
    "    # normalize amp\n",
    "    data = librosa.util.normalize(data)\n",
    "\n",
    "    # pad/truncate\n",
    "    max_length = int(target_sr * duration)\n",
    "    if len(data) > max_length:\n",
    "        data = data[:max_length]  # Truncate\n",
    "    else:\n",
    "        data = np.pad(data, (0, max_length - len(data)))  # Pad\n",
    "\n",
    "    return data, target_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = data_df.loc[0, 'File_Path']\n",
    "original_data, sr = load_audio(sample_file)\n",
    "\n",
    "print(\"Original\")\n",
    "audio_player = visualize_and_play(original_data, sr, title=\"Original Version\", size=(14,4))\n",
    "display(audio_player)\n",
    "plot_spectrogram(original_data, sr, title=\"Original Version\", size=(4,2))\n",
    "\n",
    "preprocessed_version, sr = preprocess_audio(original_data, sr)\n",
    "\n",
    "print(\"Preprocessed\")\n",
    "audio_player = visualize_and_play(preprocessed_version, sr, title=\"Preprocessed Version\", size=(14,4))\n",
    "display(audio_player)\n",
    "plot_spectrogram(preprocessed_version, sr, title=\"Preprocessed Version\", size=(4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_df, target_sr=16000, duration=2.5, batch_size=500):\n",
    "    \"\"\"\n",
    "    preprocess the dataset in batch_size.\n",
    "    \"\"\"\n",
    "    preprocessed_audio = []\n",
    "    for start in range(0, len(data_df), batch_size):\n",
    "        batch = data_df.iloc[start:start + batch_size]\n",
    "        for audio_data in batch['Audio_Data']:\n",
    "            processed_audio, _ = preprocess_audio(audio_data, sr=target_sr, target_sr=target_sr, duration=duration)\n",
    "            preprocessed_audio.append(processed_audio)\n",
    "    return np.array(preprocessed_audio)\n",
    "\n",
    "\n",
    "X_preprocessed = preprocess_dataset(full_data_df, target_sr=16000, duration=2.5)\n",
    "y = full_data_df['Emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_preprocessed:\")\n",
    "display(X_preprocessed)\n",
    "print(\"y:\")\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run optionally\n",
    "#np.save('X_preprocessed.npy', X_preprocessed)\n",
    "#np.save('y_labels.npy', y)\n",
    "\n",
    "# load preprocessed data when needed\n",
    "#X_preprocessed = np.load('X_preprocessed.npy')\n",
    "#y = np.load('y_labels.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(data, sr=16000, n_mels=128, fmax=8000, hop_length=512):\n",
    "    \"\"\"\n",
    "    Convert audio to mel-spectogram\n",
    "\n",
    "    data: one audio\n",
    "    sr: sample rate\n",
    "    n_mels: number of mel bands\n",
    "    fmax: max frequency thats included\n",
    "    hop_length: hop length\n",
    "\n",
    "    returns a mel-spectogram for the specific soundbite provided\n",
    "\n",
    "    if this is confusing, search up what they mean! but dw not too important rn.\n",
    "    \"\"\"\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=n_mels, fmax=fmax, hop_length=hop_length)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return mel_spectrogram_db\n",
    "\n",
    "X_features = np.array([extract_mel_spectrogram(data, sr=16000) for data in X_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = onehot_encoder.fit_transform(y_encoded.reshape(-1, 1))\n",
    "\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_df.shape)\n",
    "display(full_data_df.shape)\n",
    "display(X_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_features, y_onehot, test_size=0.2, stratify=y_onehot, random_state=42\n",
    ")\n",
    "\n",
    "# Then, split training+validation into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42\n",
    ")\n",
    "\n",
    "# Add the channel dimension for CNN input\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "display(X_train.shape)\n",
    "display(X_val.shape)\n",
    "display(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "The model we want to build is a Convolutional Recurrent Neural Network that uses bidirectional LSTM as well as an attention mechanism.\n",
    "\n",
    "Let's see how this goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mechanism(inputs):\n",
    "    attention = Dense(1, activation='tanh')(inputs)  # Compute attention scores\n",
    "    attention = Flatten()(attention)  # Flatten scores\n",
    "    attention = Activation('softmax')(attention)  # Normalize scores\n",
    "    attention = Lambda(lambda x: K.expand_dims(x, axis=-1))(attention)  # Expand dims\n",
    "    weighted_inputs = Multiply()([inputs, attention])  # Apply weights\n",
    "    return weighted_inputs\n",
    "\n",
    "# 3. Build the Model\n",
    "input_shape = (128, 79, 1)  # Mel-spectrogram dimensions\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional Layers\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Reshape for Recurrent Layers\n",
    "x = Reshape((x.shape[1], -1))(x)\n",
    "\n",
    "# Recurrent Layers\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Attention Mechanism\n",
    "x = attention_mechanism(x)\n",
    "\n",
    "# Flatten for Dense Layers\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully Connected Layers\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output Layer\n",
    "outputs = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# 4. Train the Model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.random.normal([10000, 10000])\n",
    "b = tf.matmul(a, a)\n",
    "print(b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
