\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Introduction}
Every day, millions of people interact with digital devices that can do amazing 
things - such as playing music, answering questions, or helping us stay organized. 
Yet although these technologies play a huge part in our lives, they are unable to 
understand how we humans feel. 

For this reason, we strive to develop a deep learning model that can classify 
emotions in human speech. This would allow us to have more enhanced 
human-machine interactions. For example, an online music playlist could 
change its song choice by adapting to the user's emotions in real-time. Another 
example is that virtual assistants could respond to users based on their 
emotional state. Being able to offer mental health support to people at their 
lowest point can potentially save lives and prevent disasters such as school 
shootings or suicides.

Moreover, as it currently stands, humans are inevitably becoming more reliant on 
digital interfaces within each passing year. For this reason, the ability to
detect emotions is essential for the future of humanity if we want to create
more enriched lifestyles for the majority of the populace.

To achieve this, our general approach is to first preprocess our data by removing
background noise and converting the audio to visual spectrograms. Then, our approach
consists of utilising a CRNN to further analyze audio data. This is because a CRNN
allows us to automate the feature extraction process and it can also easily handle
the complexities of temporal audio patterns due to its recurrent layers. Moreover,
many previous studies have shown that CRNNs perform well in similar tasks of audio
recognition.

Furthermore, a

\end{document}