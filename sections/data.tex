\documentclass[../main.tex]{subfiles}
\usepackage{hyperref}

\begin{document}
\section{Data}
The Toronto Emotional Speech Set (TESS) is a well-established dataset
designed for emotion recognition in speech. The dataset consists of 2,800
audio samples of 200 target words spoken in seven distinct emotions
(angry, disgust, fear, happy, pleasant, surprised, sad, and neutral)
by two actors: a younger adult (YAF) and an older adult (OAF).
This creates a balanced dataset with 400 samples per emotion and 1,400
samples per speaker. This balance is crucial for training unbiased models,
as it ensures equal representation of each emotional class.
\begin{figure}[h]
    \centering
    \includegraphics[width= 350pt]{tess_analysis.png}
    \caption{\\Figure 1: Comparison of waveforms and spectrograms of the word ``back'' spoken with angry, sad, and neutral emotions by the older adult female speaker (OAF). Left column shows the time-domain waveforms, the right column shows the corresponding spectrograms with intensity in dB.}
    \label{fig:analysis}
\end{figure}
\hyperref[fig:analysis]{Figure 1} reveals several key patterns in how emotions affect
speech characteristics. The waveforms show clear differences in amplitude, with
angry speech having the highest amplitude (±0.3), while sad and neutral speech
show much lower amplitudes (±0.06 and ±0.04 respectively). In the spectrograms,
we can observe that angry speech displays more intense energy
(shown by brighter colors) especially in the 0-5 kHz range, while sad and
neutral expressions show more diffused energy patterns.

The temporal characteristics also vary notably across emotions. As seen in the
time axes, the sad utterance is the longest (around 2.5 seconds), while the
angry utterance is more compact (1.5 seconds), and the neutral utterance takes
a moderate duration (2 seconds). Looking at the spectrograms' frequency
distribution, angry speech shows distinct, high-intensity patterns
(visible as bright red-purple regions), whereas sad and neutral expressions
share similar, more subtle spectral patterns. These acoustic similarities
between certain emotion pairs, combined with varying temporal patterns,
suggest our model will need to consider both timing and frequency
characteristics to effectively classify emotions.


\end{document}
